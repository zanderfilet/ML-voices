{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn \n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset_generation.randomDataset import RandomDataset\n",
    "from classifier.dataset import SoundDataset\n",
    "from utils.cnn import CNNNetwork\n",
    "from utils.lstm import LSTMNetwork\n",
    "from utils import LABELS\n",
    "from utils.numpyDataset import NumpyDataset\n",
    "from encodec.utils import convert_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n",
    "sr = 8000\n",
    "num_samples = 12000\n",
    "nfft=512\n",
    "nmels=60\n",
    "f_min=0\n",
    "f_max=None\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 150\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories =  [\n",
    " #'audioMnist-pitch-up',\n",
    " 'audioMinst',\n",
    " #'audioMnistPitchShiftedDown',\n",
    " #'audioMnistNoise',\n",
    " 'jenniferMnist'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12002\n",
      "shape of feature: torch.Size([1, 60, 47])\n",
      "datatype torch.float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "melSpec = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=nfft, n_mels=nmels, f_min=f_min, f_max=f_max)\n",
    "def transform(wav):\n",
    "    a1 = random.sample(augmentations, 1)[0]\n",
    "    wav = aug.apply(wav, a1)\n",
    "    spec = mel\n",
    "    return melSpec(wav)\n",
    "\n",
    "dataSet = SoundDataset(directories, transformation=melSpec, target_sample_rate=sr, num_samples=num_samples, device=device)\n",
    "print(len(dataSet))\n",
    "train_data, val_data = torch.utils.data.random_split(dataSet, (0.8, 0.2))\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "input_shape = None\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3), sharex=True, sharey=True)\n",
    "for i in range(3):\n",
    "    img, label = val_data[i]\n",
    "    if i==0:\n",
    "        print('shape of feature:', img.shape)\n",
    "        print('datatype', img.dtype)\n",
    "        input_shape = img.shape\n",
    "    axs[i].imshow(img[0].cpu(), origin='lower')\n",
    "    axs[i].set_title(f'label: {label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852992\n",
      "torch.Size([1, 1, 60, 188])\n",
      "torch.Size([1, 8, 31, 95])\n",
      "torch.Size([1, 16, 16, 48])\n",
      "torch.Size([1, 32, 9, 25])\n",
      "torch.Size([1, 64, 5, 13])\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (4): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = CNNNetwork(out_neurons=10, input_shape=(1, 60, 188), layers=[8, 16, 32, 64], stride=(1, 1))\n",
    "model2(torch.rand(1, 1, 60, 188))\n",
    "model2 = model2.to(device)\n",
    "#model = LSTMNetwork(inp#ut_dim=8, hidden_dim=32, lstm_layers=1, linear_layers=[16, 10])\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "# Set all parameters in the model to non-trainable (requires_grad=False)\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "# Pass your Mel spectrogram through the model\n",
    "\n",
    "# deletes the last layer of resnet and replace it with two of our own linear layers\n",
    "resnet18 = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
    "new_fc =  torch.nn.Linear(512, 256)\n",
    "new_fc2 = torch.nn.Linear(256, 10)\n",
    "new_layers = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    new_fc,\n",
    "    nn.ReLU(),\n",
    "    new_fc2,\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "resnet18_modified = nn.Sequential(\n",
    "    resnet18,\n",
    "    new_layers\n",
    ")\n",
    "\n",
    "model = resnet18_modified.to(device)\n",
    "print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_data_loader, val_data_loader, loss_fn, opt, device, metrics):\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for input, target in train_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        input = torch.cat((input, input, input), dim=1)\n",
    "        #input = torch.cat((input, input, input, input), dim=3)\n",
    "        # calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        _, preds = torch.max(prediction.data, 1)\n",
    "        accuracies.append((preds == target).float().mean())\n",
    "        losses.append(loss.item())\n",
    "        # backpropagate error and update weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    metrics['t_acc'].append((sum(accuracies)/len(accuracies)).cpu())\n",
    "    metrics['t_loss'].append((sum(losses)/sum(accuracies)).cpu())\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for input, target in val_data_loader:\n",
    "        input = torch.cat((input, input, input), dim=1)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        #nput = torch.cat((input, input, input, input), dim=3)\n",
    "\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        _, preds = torch.max(prediction.data, 1)\n",
    "        accuracies.append((preds == target).float().mean())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    metrics['v_acc'].append((sum(accuracies)/len(accuracies)).cpu())\n",
    "    metrics['v_loss'].append((sum(losses)/sum(accuracies)).cpu())\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** STARTING TRAINING ******\n",
      "Finished epoch: 0 Val loss: 16.936359405517578, Val acc: 0.1352796107530594\n",
      "Finished epoch: 1 Val loss: 13.357015609741211, Val acc: 0.17023026943206787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m****** STARTING TRAINING ******\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_acc\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFininshed training\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, opt, device, metrics)\u001b[0m\n\u001b[1;32m      4\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;129;01min\u001b[39;00m train_data_loader:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28minput\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/voculator/style_transfer/../classifier/dataset.py:47\u001b[0m, in \u001b[0;36mSoundDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     45\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cut_if_necessary(signal)\n\u001b[1;32m     46\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_right_pad_if_necessary(signal)\n\u001b[0;32m---> 47\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signal, label\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torchaudio/transforms/_transforms.py:650\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    643\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torchaudio/functional/functional.py:126\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_length_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n\u001b[1;32m    140\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mreshape(shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/torch/functional.py:641\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    642\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    't_loss': [],\n",
    "    'v_loss': [],\n",
    "    't_acc': [],\n",
    "    'v_acc': []\n",
    "}\n",
    "print('****** STARTING TRAINING ******')\n",
    "for epoch in range(50):\n",
    "    train_epoch(model, train_dl, val_dl, loss_fn=loss, opt=optimizer, device=device, metrics=metrics)\n",
    "    print(f\"Finished epoch: {epoch} Val loss: {metrics['v_loss'][-1]}, Val acc: {metrics['v_acc'][-1]}\")\n",
    "\n",
    "print(\"Fininshed training\")    \n",
    "print('best validation accuracy', max(metrics['v_acc']).data)\n",
    "\n",
    "plt.plot(metrics['t_loss'])\n",
    "plt.plot(metrics['v_loss'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.title('cross entropy loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(metrics['t_acc'])\n",
    "plt.plot(metrics['v_acc'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.title('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE THE MODEL, INCLUDING THE SPECOGRAM PARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
