{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set model hyperparameters and feature hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "sr = 8000\n",
    "num_samples = 12000\n",
    "nfft=1024\n",
    "nmels=40\n",
    "f_min=0\n",
    "f_max=None\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose a feature to transform the data to and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4_theo_16.wav',\n",
       " '3_yweweler_18.wav',\n",
       " '3_yweweler_10.wav',\n",
       " '3_nicolas_45.wav',\n",
       " '6_yweweler_43.wav']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = os.listdir(\"../../datasets/audioMinst\")\n",
    "all_files[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_files = []\n",
    "trim_files = []\n",
    "sr = 8000\n",
    "for file in all_files:\n",
    "    og_file, rate = torchaudio.load(\"../../datasets/audioMinst/\" + file, normalize=True)\n",
    "    og_files.append(og_file)\n",
    "    sr = rate\n",
    "    trim_file = torch.cat((torch.zeros(1, 2000), og_file[:, 2000:]), dim=1)\n",
    "    trim_files.append(trim_file)\n",
    "    filename = file.split('.')[0]\n",
    "    torchaudio.save('../../datasets/trimming/' + filename + '_trim.wav', trim_file, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Audio(og_files[1],rate = sr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,UklGRnAWAABXQVZFZm10IBAAAAABAAEAQB8AAIA+AAACABAAZGF0YUwWAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcDE5MNgfHh34/FDlJdYzrSqqS5GGgYKSAYBxjKyhga3sytrfqvyfFKQoMzdPSndPG1MyUk1ABTkNF6gXovk55groOsuB0ojLgdLg2D/fkfN0+9QLFRqcJX8tni/AO0o2PDr0MvIo6SVMGzsVvQwfAh8C+Pxr+Gv4Rv1d/FYD+QZHB2kTOAv2F3oZlxF0IAoNWA0hDKr8Jvvr5XXglsfAsWDGD5eHpomw2p99vh643s6V4jLtfgiXEW4nxCpbPFM5WTLVMDcmax3LCPj8+PwI3jnmdeD31z/fKOCg75HzhAGAElIUJiA3Jp4vni++MSs0xCoAJdgf9hdgECEMGQnoAHz+a/h8/ov6yv5NADYBMAiVB6YNMAhBDvMNJwVgELoCL/74/Ln4Q/Om6HvZZr+p1zuwBa9Ryuul28TKvknHm9tc16L5hAHLCEwbPyn6K9UwKCryKOklnxSuEMsI0Pff887tKupn5Kbo7e+o8tD3wwVvDO0UghwAJVcoSCy1LiIx2ymzJOklNRyIFYASIQw/BLoC2fpU+WP1sfXQ9x74B/lG/cH7jAQ/BH4Iywg/BDAIwwUIA/EDL/6T/cj0JPFW3snZdeDqwPXNEsarvBLGvMLm0ZnRCN5x8bH1RwcEFF0hACXMLSs0Ay+1Ll8rACXHGSoPGQk2AaL50Pex9Q3ysfV0+7oCRwfcDscZ0Bz4IQAlCShlJF0hwSCoF70MlQd8/gf5l+yJ8OvlMOMC5QLlzu1x8YnwovkY/3T7Rwc/BKMDywheBj8EGP/4/Ln4Q/Nx8fzreOqj3jbcut3310XYTdsX2jbcP9+d5dzpoO89+lYDowMhDLcTtxNaF0wbPR81HIIcFRqRGM4StxOfFPMNQQ5BDoYLnQpPCvMNIQymDUkRSRG3E5cRUhSuELQJ3A7DBegA6ADI9HT7ifDt73r0pugk8WnurutD82nuDfL+9R74VPld/Pj8D/zK/pP9AACbANn6+PzI9Hr0KurF6mfk8d605B/dm9vr5VDl/Ot69KL5hAHUC1IUehlMG4sfqiEPISYg2B96GTsVaRPUC34I+QajA9oE2gRtAkcH2gS0CSEMSRFJEcUPSRG3E8UPbwz8ECcFugI2AUz28Pn28jvw1vDc6cXqBO/l7ATvLPTI9Hr0HvjZ+qr8HwJtAugACAPh/ar8qvxU+cH7WvJ69A3yneXO7Xjqvef86yrqBO+g7/718PnRAdEBTwoTENwOOxUNFzsV9hcNF3EW1hUyEq4QWA0CCkcH4gdtAowEEAYIA3UFPwSsBsMFXgasBicFEAasBj8EjAS6Ak0AJvvn9tbwevRx8Xjqae5x8VjoO/Co8hvu9vJU+dn6JvuEAaMDjATDBYYLJwVPCl4G2gRtAi/+PfoH+Zn2sfWx9Sz0Y/Vj9dD3i/pr+D36fP7oAHUFrAZPCoYLAgrFD70MhguGCwIKhgteBn4IRwfDBYwEEAbaBOIHhgt+CPMNvQyGCxAG4geVB7oCEAZNAIv6Pfrw+Zn2ufhM9lT5gvff88H7Hvix9Ub94f1r+Eb9TQBd/A/8k/0P/Cb7D/zZ+pP9i/r4/Bj/TPbB+yb7B/m5+Nn68PmZ9lT5Rv0m+4v66ADh/bP/CAMAABAGjAQQBvkG4gd+CPEDHwJWAwgDNgF1BT8EmwDDBT8EdQU/BGcJXgZ+COIHXgbxAz8EJwW6Al38JvtNAFry8Pld/Of2yPTw+d/zcfGL+pn2yPTB+4v6wfttAkb9fP6bAMr+ugI/BAgDowOMBHz+s//xAxj/ugKdCuIH8QMwCHUFNgHoALoCZf98/tn6k/2x9cj0a/iZ9rH1B/kH+XT7dPvxA5sANgEwCOIHdQUwCDAIrAb5BssIhguVB8MFtAmz/y/+ugKEAbP/yv5WA6r8sfUAAF38sfXK/psAufgAAEb98Pm5+Pj8D/wH+Yv6Rv1U+fj8k/0m+5sANgFl/1YDRv3aBCcFfghHB8sIowMnBV4Gs/+MBIwE+PwIA2X/s/9NACb7TQDh/Qf5k/3Z+h74Jvvw+YL3/vWi+bn48Pl0+0b96ADK/vEDfgjxA0cHywjxAwgDfgg/BA/8wwW6AjX3NgFNAPD5yv5l/9n6Pfqi+T36B/lr+MH7ovk19yb7k/2C9wAANgF8/nUFAgo/BDgLlQfaBPkGwwWz/1YDTQBG/cr++PyT/Ub9D/zh/T36ufjh/Qf5ovll//D5dPvh/dn6L/5G/Sb7+Pz4/OH98QMAAIQBlQc/BLoCXgYIA6MDwwWbAAgDTQCjA9oEowNtAg==\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Audio(trim_files[1],rate = sr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /home/aneri/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of feature: torch.Size([1, 10, 24])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "melSpec = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=nfft, n_mels=nmels, f_min=f_min, f_max=f_max)\n",
    "mfcc = torchaudio.transforms.MFCC(sample_rate=sr, n_mfcc=10, melkwargs={'n_fft':nfft, 'n_mels':nmels})\n",
    "codec = CodecTransform(sample_rate=sr, bandwidth=6.0)\n",
    "\n",
    "dataSet = SoundDataset('audioMinst', transformation=mfcc, target_sample_rate=sr, num_samples=num_samples)\n",
    "train_data, val_data = torch.utils.data.random_split(dataSet, (2500, 500))\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size)\n",
    "input_shape = None\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3), sharex=True, sharey=True)\n",
    "for i in range(3):\n",
    "    img, label = val_data[i]\n",
    "    if i==0:\n",
    "        print('shape of feature:', img.shape)\n",
    "        input_shape = img.shape\n",
    "    axs[i].imshow(img[0], origin='lower')\n",
    "    axs[i].set_title(f'label: {label}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNNetwork(\n",
      "  (conv_layer0): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer1): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=384, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNNNetwork(out_neurons=10, input_shape=input_shape, layers=[8, 16, 32, 64])\n",
    "#model = LSTMNetwork(input_dim=8, lstm_layers=[32, 24], linear_layers=[16, 10])\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_data_loader, val_data_loader, loss_fn, opt, device, metrics):\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for input, target in train_data_loader:\n",
    "        input, target = input.float().to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        _, preds = torch.max(prediction.data, 1)\n",
    "        accuracies.append((preds == target).float().mean())\n",
    "        losses.append(loss.item())\n",
    "        # backpropagate error and update weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    metrics['t_acc'].append(sum(accuracies)/len(accuracies))\n",
    "    metrics['t_loss'].append(sum(losses)/sum(accuracies))\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for input, target in val_data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        _, preds = torch.max(prediction.data, 1)\n",
    "        accuracies.append((preds == target).float().mean())\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    metrics['v_acc'].append(sum(accuracies)/len(accuracies))\n",
    "    metrics['v_loss'].append(sum(losses)/sum(accuracies))\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** STARTING TRAINING ******\n",
      "torch.Size([64, 1, 10, 24])\n",
      "torch.Size([64, 8, 6, 13])\n",
      "torch.Size([64, 16, 4, 7])\n",
      "torch.Size([64, 32, 3, 4])\n",
      "torch.Size([64, 64, 2, 3])\n",
      "Finished epoch: 0 Val loss: 22.79452896118164\n",
      "Finished epoch: 1 Val loss: 15.1439790725708\n",
      "Finished epoch: 2 Val loss: 13.809484481811523\n",
      "Finished epoch: 3 Val loss: 11.21994686126709\n",
      "Finished epoch: 4 Val loss: 9.394502639770508\n",
      "Finished epoch: 5 Val loss: 6.7848405838012695\n",
      "Finished epoch: 6 Val loss: 6.22121524810791\n",
      "Finished epoch: 7 Val loss: 5.679296970367432\n",
      "Finished epoch: 8 Val loss: 5.265852451324463\n",
      "Finished epoch: 9 Val loss: 5.087989330291748\n",
      "Finished epoch: 10 Val loss: 4.610058784484863\n",
      "Finished epoch: 11 Val loss: 4.429529666900635\n",
      "Finished epoch: 12 Val loss: 4.345503807067871\n",
      "Finished epoch: 13 Val loss: 4.242002487182617\n",
      "Finished epoch: 14 Val loss: 4.147792339324951\n",
      "Finished epoch: 15 Val loss: 4.0593342781066895\n",
      "Finished epoch: 16 Val loss: 3.9456779956817627\n",
      "Finished epoch: 17 Val loss: 3.811037063598633\n",
      "Finished epoch: 18 Val loss: 3.766561269760132\n",
      "Finished epoch: 19 Val loss: 3.6937155723571777\n",
      "Finished epoch: 20 Val loss: 3.669387102127075\n",
      "Finished epoch: 21 Val loss: 3.5787386894226074\n",
      "Finished epoch: 22 Val loss: 3.55600643157959\n",
      "Finished epoch: 23 Val loss: 3.573369026184082\n",
      "Finished epoch: 24 Val loss: 3.5658915042877197\n",
      "Finished epoch: 25 Val loss: 3.560171604156494\n",
      "Finished epoch: 26 Val loss: 3.582132577896118\n",
      "Finished epoch: 27 Val loss: 3.563115119934082\n",
      "Finished epoch: 28 Val loss: 3.602844476699829\n",
      "Finished epoch: 29 Val loss: 3.6003003120422363\n",
      "Finished epoch: 30 Val loss: 3.5825395584106445\n",
      "Finished epoch: 31 Val loss: 3.467902898788452\n",
      "Finished epoch: 32 Val loss: 3.142024517059326\n",
      "Finished epoch: 33 Val loss: 3.0337865352630615\n",
      "Finished epoch: 34 Val loss: 2.993957757949829\n",
      "Finished epoch: 35 Val loss: 2.9401140213012695\n",
      "Finished epoch: 36 Val loss: 2.9090187549591064\n",
      "Finished epoch: 37 Val loss: 2.89945125579834\n",
      "Finished epoch: 38 Val loss: 2.8946096897125244\n",
      "Finished epoch: 39 Val loss: 2.865593671798706\n",
      "Finished epoch: 40 Val loss: 2.853862762451172\n",
      "Finished epoch: 41 Val loss: 2.8235719203948975\n",
      "Finished epoch: 42 Val loss: 2.647373914718628\n",
      "Finished epoch: 43 Val loss: 2.6333274841308594\n",
      "Finished epoch: 44 Val loss: 2.5477426052093506\n",
      "Finished epoch: 45 Val loss: 2.479414463043213\n",
      "Finished epoch: 46 Val loss: 2.460787296295166\n",
      "Finished epoch: 47 Val loss: 2.4328057765960693\n",
      "Finished epoch: 48 Val loss: 2.4252700805664062\n",
      "Finished epoch: 49 Val loss: 2.419584035873413\n",
      "Finished epoch: 50 Val loss: 2.3740651607513428\n",
      "Finished epoch: 51 Val loss: 2.3567066192626953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m****** STARTING TRAINING ******\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFininshed training\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, opt, device, metrics)\u001b[0m\n\u001b[1;32m      4\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;129;01min\u001b[39;00m train_data_loader:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/voculator/classifier/dataset.py:39\u001b[0m, in \u001b[0;36mSoundDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     37\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cut_if_necessary(signal)\n\u001b[1;32m     38\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_right_pad_if_necessary(signal)\n\u001b[0;32m---> 39\u001b[0m signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signal, label\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:722\u001b[0m, in \u001b[0;36mMFCC.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    715\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;124;03m        Tensor: specgram_mel_db of size (..., ``n_mfcc``, time).\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMelSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_mels:\n\u001b[1;32m    724\u001b[0m         log_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:643\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrogram(waveform)\n\u001b[0;32m--> 643\u001b[0m mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecgram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:410\u001b[0m, in \u001b[0;36mMelScale.forward\u001b[0;34m(self, specgram)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    specgram (Tensor): A spectrogram STFT of dimension (..., freq, time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# (..., time, freq) dot (freq, n_mels) -> (..., n_mels, time)\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecgram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "    't_loss': [],\n",
    "    'v_loss': [],\n",
    "    't_acc': [],\n",
    "    'v_acc': []\n",
    "}\n",
    "print('****** STARTING TRAINING ******')\n",
    "for epoch in range(num_epochs):\n",
    "    train_epoch(model, train_dl, val_dl, loss_fn=loss, opt=optimizer, device=device, metrics=metrics)\n",
    "    print(f\"Finished epoch: {epoch} Val loss: {metrics['v_loss'][-1]}\")\n",
    "\n",
    "print(\"Fininshed training\")    \n",
    "print('best validation accuracy', max(metrics['v_acc']).data)\n",
    "\n",
    "plt.plot(metrics['t_loss'])\n",
    "plt.plot(metrics['v_loss'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.title('cross entropy loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(metrics['t_acc'])\n",
    "plt.plot(metrics['v_acc'])\n",
    "plt.legend(['training', 'validation'])\n",
    "plt.title('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the data on an unseen dataset, my voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label was 1. Model Classified as 1\n",
      "label was 0. Model Classified as 0\n",
      "label was 6. Model Classified as 6\n",
      "label was 8. Model Classified as 8\n",
      "label was 7. Model Classified as 7\n",
      "label was 3. Model Classified as 8\n",
      "label was 5. Model Classified as 9\n",
      "label was 4. Model Classified as 4\n",
      "label was 2. Model Classified as 2\n",
      "label was 9. Model Classified as 9\n",
      "test accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "testDataSet = SoundDataset('dylanMnist', transformation=mfcc, target_sample_rate=sr, num_samples=num_samples)\n",
    "testDL = DataLoader(testDataSet, batch_size=1)\n",
    "correct = 0\n",
    "model.eval()\n",
    "for inp, target in testDL:\n",
    "    inp, target = inp.to(device), target.to(device)\n",
    "    prediction = model(inp)\n",
    "    _, preds = torch.max(prediction.data, 1)\n",
    "    if target[0] == preds[0]:\n",
    "        correct += 1\n",
    "    print(f'label was {target[0]}. Model Classified as {preds[0]}')\n",
    "    \n",
    "print(f'test accuracy: {correct/10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "40eda13ed9a4de3a0e0630323248f47ce4d9ddf1e72ea47c87ee6f84d1644212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
