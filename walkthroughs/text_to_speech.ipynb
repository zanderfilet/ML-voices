{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Text-to-Speech with Tacotron2\n",
    "\n",
    "**Author**: [Yao-Yuan Yang](https://github.com/yangarbiter)_,\n",
    "[Moto Hira](moto@meta.com)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lg/fl8dfycj3hx7f_kqhqw67zvh0000gn/T/ipykernel_49829/3301642954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSoundDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"figure.figsize\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial shows how to build text-to-speech pipeline, using the\n",
    "pretrained Tacotron2 in torchaudio.\n",
    "\n",
    "The text-to-speech pipeline goes as follows:\n",
    "\n",
    "1. Text preprocessing\n",
    "\n",
    "   First, the input text is encoded into a list of symbols. In this\n",
    "   tutorial, we will use English characters and phonemes as the symbols.\n",
    "\n",
    "2. Spectrogram generation\n",
    "\n",
    "   From the encoded text, a spectrogram is generated. We use ``Tacotron2``\n",
    "   model for this.\n",
    "\n",
    "3. Time-domain conversion\n",
    "\n",
    "   The last step is converting the spectrogram into the waveform. The\n",
    "   process to generate speech from spectrogram is also called Vocoder.\n",
    "   In this tutorial, three different vocoders are used,\n",
    "   :py:class:`~torchaudio.models.WaveRNN`,\n",
    "   :py:class:`~torchaudio.transforms.GriffinLim`, and\n",
    "   [Nvidia's WaveGlow](https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/)_.\n",
    "\n",
    "\n",
    "The following figure illustrates the whole process.\n",
    "\n",
    "<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/tacotron2_tts_pipeline.png\">\n",
    "\n",
    "All the related components are bundled in :py:class:`torchaudio.pipelines.Tacotron2TTSBundle`,\n",
    "but this tutorial will also cover the process under the hood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the above, the symbol table and indices must match\n",
    "what the pretrained Tacotron2 model expects. ``torchaudio`` provides the\n",
    "transform along with the pretrained model. For example, you can\n",
    "instantiate and use such transform as follow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phoneme-based encoding\n",
    "\n",
    "Phoneme-based encoding is similar to character-based encoding, but it\n",
    "uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme)\n",
    "model.\n",
    "\n",
    "The detail of the G2P model is out of scope of this tutorial, we will\n",
    "just look at what the conversion looks like.\n",
    "\n",
    "Similar to the case of character-based encoding, the encoding process is\n",
    "expected to match what a pretrained Tacotron2 model is trained on.\n",
    "``torchaudio`` has an interface to create the process.\n",
    "\n",
    "The following code illustrates how to make and use the process. Behind\n",
    "the scene, a G2P model is created using ``DeepPhonemizer`` package, and\n",
    "the pretrained weights published by the author of ``DeepPhonemizer`` is\n",
    "fetched.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "\n",
    "text = ['hello', 'world', 'this is long', 'stupid']\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "\n",
    "print(processed)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Generation\n",
    "\n",
    "``Tacotron2`` is the model we use to generate spectrogram from the\n",
    "encoded text. For the detail of the model, please refer to [the\n",
    "paper](https://arxiv.org/abs/1712.05884)_.\n",
    "\n",
    "It is easy to instantiate a Tacotron2 model with pretrained weight,\n",
    "however, note that the input to Tacotron2 models need to be processed\n",
    "by the matching text processor.\n",
    "\n",
    ":py:class:`torchaudio.pipelines.Tacotron2TTSBundle` bundles the matching\n",
    "models and processors together so that it is easy to create the pipeline.\n",
    "\n",
    "For the available bundles, and its usage, please refer to\n",
    ":py:class:`~torchaudio.pipelines.Tacotron2TTSBundle`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "\n",
    "text = \"Big trees\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, _, _ = tacotron2.infer(processed, lengths)\n",
    "\n",
    "\n",
    "_ = plt.imshow(spec[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveform Generation\n",
    "\n",
    "Once the spectrogram is generated, the last process is to recover the\n",
    "waveform from the spectrogram.\n",
    "\n",
    "``torchaudio`` provides vocoders based on ``GriffinLim`` and\n",
    "``WaveRNN``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveRNN\n",
    "\n",
    "Continuing from the previous section, we can instantiate the matching\n",
    "WaveRNN model from the same bundle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "text = \"Hello world\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    print(lengths)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    waveforms, lengths = vocoder(spec, spec_lengths)\n",
    "\n",
    "    print(processed.shape)\n",
    "    print(spec.shape)\n",
    "    print(spec_lengths)\n",
    "    print(waveforms.shape)\n",
    "    print(lengths)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))\n",
    "ax1.imshow(spec[0].cpu().detach())\n",
    "ax2.plot(waveforms[0].cpu().detach())\n",
    "\n",
    "IPython.display.Audio(waveforms[0:1].cpu(), rate=vocoder.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Griffin-Lim\n",
    "\n",
    "Using the Griffin-Lim vocoder is same as WaveRNN. You can instantiate\n",
    "the vocode object with\n",
    ":py:func:`~torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder`\n",
    "method and pass the spectrogram.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# bundle = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH\n",
    "\n",
    "# processor = bundle.get_text_processor()\n",
    "# tacotron2 = bundle.get_tacotron2().to(device)\n",
    "# vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "text = \"I am a robot\"\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "waveforms, lengths = vocoder(spec, spec_lengths)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))\n",
    "ax1.imshow(spec[0].cpu().detach())\n",
    "ax2.plot(waveforms[0].cpu().detach())\n",
    "\n",
    "IPython.display.Audio(waveforms[0:1].cpu(), rate=vocoder.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveglow\n",
    "\n",
    "Waveglow is a vocoder published by Nvidia. The pretrained weights are\n",
    "published on Torch Hub. One can instantiate the model using ``torch.hub``\n",
    "module.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Workaround to load model mapped on GPU\n",
    "# https://stackoverflow.com/a/61840832\n",
    "waveglow = torch.hub.load(\n",
    "    \"NVIDIA/DeepLearningExamples:torchhub\",\n",
    "    \"nvidia_waveglow\",\n",
    "    model_math=\"fp32\",\n",
    "    pretrained=False,\n",
    ")\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "    \"https://api.ngc.nvidia.com/v2/models/nvidia/waveglowpyt_fp32/versions/1/files/nvidia_waveglowpyt_fp32_20190306.pth\",  # noqa: E501\n",
    "    progress=False,\n",
    "    map_location=device,\n",
    ")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in checkpoint[\"state_dict\"].items()}\n",
    "\n",
    "waveglow.load_state_dict(state_dict)\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.to(device)\n",
    "waveglow.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    waveforms = waveglow.infer(spec)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))\n",
    "ax1.imshow(spec[0].cpu().detach())\n",
    "ax2.plot(waveforms[0].cpu().detach())\n",
    "\n",
    "IPython.display.Audio(waveforms[0:1].cpu(), rate=22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "40eda13ed9a4de3a0e0630323248f47ce4d9ddf1e72ea47c87ee6f84d1644212"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
